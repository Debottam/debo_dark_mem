{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "generate_data.py \n",
    "\n",
    "Script to generate an HDF5 archive of the MEM data.\n",
    "The data will be split into 3 datasets:\n",
    "    A) used for training the soft target and llhs regression tasks\n",
    "    B) used for testing the soft target and llhs regression tasks,\n",
    "       as well as training the hard target classification tasks\n",
    "    C) used for testing the hard target classification tasks\n",
    "The archives will contain:\n",
    "    X\n",
    "    y\n",
    "    soft_target\n",
    "    blk_llh\n",
    "    signal_llh\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 7103 on context None\n",
      "Mapped name None to device cuda0: GeForce GTX 1080 (0000:02:00.0)\n"
     ]
    }
   ],
   "source": [
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/solved/.virtualenvs/MEMDK/local/lib/python2.7/site-packages/IPython/kernel/__init__.py:13: ShimWarning: The `IPython.kernel` package has been deprecated since IPython 4.0.You should import from ipykernel or jupyter_client instead.\n",
      "  \"You should import from ipykernel or jupyter_client instead.\", ShimWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.12/06\n"
     ]
    }
   ],
   "source": [
    "%run Toolkit_particle.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%run Toolkit_utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run Plot_distributions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import deepdish.io as io\n",
    "from tqdm import tqdm\n",
    "import keras\n",
    "from tqdm import tqdm\n",
    "import pandautils as pup\n",
    "\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "#Changed to notebooks:\n",
    "#from toolkit.particles import Jet, Lepton\n",
    "#from toolkit.utils import parallel_run, safe_mkdir\n",
    "#from plots import plot_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -- logging stuff\n",
    "LOGGER_PREFIX = ' %s'\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "def log(msg):\n",
    "    logger.info(LOGGER_PREFIX % msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TREE = 'METree'\n",
    "#ONLY_4V = False\n",
    "# Each one of the root files has one TTree with 38 branches (variables). Out of those, we pick these:\n",
    "# -- select variables needed in this analysis, including inputs and cuts\n",
    "signal_vars = ['ME_Sig_Obj_pt', 'ME_Sig_Obj_eta', 'ME_Sig_Obj_phi', 'ME_Sig_Obj_E',\n",
    "              'ME_Sig_Likelihoods_Available',\n",
    "              'ME_Sig_Mod_Likelihoods',\n",
    "              'ME_Sig_Obj_type'] \n",
    "bkg_vars = ['ME_Bkg_Mod_Likelihoods', 'ME_Bkg_Likelihoods_Available']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Data from Olaf:\n",
    "/eos/atlas/user/n/nackenho/MEMDKOutput/truth1D/\n",
    "\n",
    "Signal sample with signal hypothesis:\n",
    "\n",
    "SA_tev13_mg5_ttH_EL_64_Sig_merge.root\n",
    "SA_tev13_mg5_ttH_MU_64_Sig_merge.root\n",
    "\n",
    "\n",
    "Signal sample with background hypothesis:\n",
    "\n",
    "SA_tev13_mg5_ttH_EL_64_Bkg_merge.root\n",
    "SA_tev13_mg5_ttH_MU_64_Bkg_merge.root\n",
    "\n",
    "\n",
    "Background sample with signal hypothesis:\n",
    "\n",
    "SA_mg5_ttbar_bjet_EL_64_Sig_merge.root\n",
    "SA_mg5_ttbar_bjet_MU_64_Sig_merge.root\n",
    "\n",
    "\n",
    "Background sample with background hypothesis:\n",
    "\n",
    "SA_mg5_ttbar_bjet_EL_64_Bkg_merge.root\n",
    "SA_mg5_ttbar_bjet_MU_64_Bkg_merge.root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data =\"type of data to use ('1d', '7d', or 'reco')\". Replaces args.data in python code.\n",
    "data = '1d'\n",
    "\n",
    "SAMPLE_PATH = os.path.join('../data', data)\n",
    "safe_mkdir(SAMPLE_PATH)\n",
    "\n",
    "higgs_signal_files = sorted([f for f in os.listdir(SAMPLE_PATH) \n",
    "    if (('Sig' in f) and (('ttH' in f)))])\n",
    "higgs_bkg_files = sorted([f for f in os.listdir(SAMPLE_PATH) \n",
    "    if (('Bkg' in f) and (('ttH' in f)))])\n",
    "ttbar_signal_files = sorted([f for f in os.listdir(SAMPLE_PATH) \n",
    "    if (('Sig' in f) and ('bjet' in f) and ('rfast' not in f) and ('PPP' not in f))])\n",
    "ttbar_bkg_files = sorted([f for f in os.listdir(SAMPLE_PATH) \n",
    "    if (('Bkg' in f) and ('bjet' in f) and ('rfast' not in f) and ('PPP' not in f))])\n",
    "\n",
    "print 'hs:', higgs_signal_files,'\\n'\n",
    "print 'hb:', higgs_bkg_files,'\\n'\n",
    "print 'ts:', ttbar_signal_files,'\\n'\n",
    "print 'tb:', ttbar_bkg_files,'\\n'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Not sure if these comments are still relevant, we haven't tried 7D in a long time.    \n",
    "\n",
    "    # don't use mu channel for 7d integration because it has different number of events\n",
    "    # if data == '7d':\n",
    "    #     higgs_signal_files = [f for f in higgs_signal_files if 'MU' not in f]\n",
    "    #     higgs_bkg_files = [f for f in higgs_bkg_files if 'MU' not in f] \n",
    "    #     ttbar_signal_files = [f for f in ttbar_signal_files if 'MU' not in f]  \n",
    "    #     ttbar_bkg_files = [f for f in ttbar_bkg_files if 'MU' not in f] \n",
    "       \n",
    "    # log('higgs signal files = {}\\n higgs bkg files = {}\\n ttbar signal files = {}\\n ttbar bkg files = {}'.format(\n",
    "    #     higgs_signal_files, higgs_bkg_files, ttbar_signal_files, ttbar_bkg_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "log('loading signal ROOT files')\n",
    "# -- load signal samples\n",
    "higgs_signal_df = pup.root2panda(\n",
    "    [os.path.join(SAMPLE_PATH, signal) for signal in higgs_signal_files], TREE,\n",
    "    branches = signal_vars\n",
    ")\n",
    "print [os.path.join(SAMPLE_PATH, signal) for signal in higgs_signal_files]\n",
    "ttbar_signal_df = pup.root2panda(\n",
    "    [os.path.join(SAMPLE_PATH, signal) for signal in ttbar_signal_files], TREE,\n",
    "    branches = signal_vars\n",
    ")\n",
    "print  [os.path.join(SAMPLE_PATH, signal) for signal in ttbar_signal_files]\n",
    "log('loading background ROOT files')\n",
    "# -- load background samples\n",
    "higgs_bkg_df = pup.root2panda(\n",
    "    [os.path.join(SAMPLE_PATH, bkg) for bkg in higgs_bkg_files], TREE,\n",
    "    branches = bkg_vars\n",
    ")\n",
    "print [os.path.join(SAMPLE_PATH, bkg) for bkg in higgs_bkg_files]\n",
    "ttbar_bkg_df = pup.root2panda(\n",
    "    [os.path.join(SAMPLE_PATH, bkg) for bkg in ttbar_bkg_files], TREE,\n",
    "    branches = bkg_vars\n",
    ")\n",
    "print  [os.path.join(SAMPLE_PATH, bkg) for bkg in ttbar_bkg_files]\n",
    "log('applying preselections')\n",
    "higgs_slice = np.logical_and(\n",
    "    higgs_signal_df['ME_Sig_Likelihoods_Available'] == True, \n",
    "    higgs_bkg_df['ME_Bkg_Likelihoods_Available'] == True\n",
    "    )\n",
    "ttbar_slice = np.logical_and(\n",
    "    ttbar_signal_df['ME_Sig_Likelihoods_Available'] == True, \n",
    "    ttbar_bkg_df['ME_Bkg_Likelihoods_Available'] == True\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sortable_cols = ['ME_Sig_Obj_pt', 'ME_Sig_Obj_eta', 'ME_Sig_Obj_phi', 'ME_Sig_Obj_E', 'ME_Sig_Obj_type']\n",
    "def sortby(column):\n",
    "    unsorted_dataframe = pd.DataFrame(data=higgs_signal_df)[::]\n",
    "    unsorted_dataframe = unsorted_dataframe[0:5]\n",
    "    sorted_dataframe = unsorted_dataframe[::]\n",
    "    for col in sortable_cols:\n",
    "        for n in range(len(unsorted_dataframe)):\n",
    "            sorter = np.argsort(higgs_signal_df[column][n])\n",
    "            col_position = higgs_signal_df.columns.get_loc(col)\n",
    "            sorted_col = [[higgs_signal_df.iloc[n,col_position][sorter[::]]]]\n",
    "            sorted_dataframe.iloc[n,col_position] = sorted_col[::]\n",
    "    return sorted_dataframe\n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sortby('ME_Sig_Obj_pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unsorted_dataframe = pd.DataFrame(data=higgs_signal_df)[::]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unsorted_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build X from those 4 branches only (pt, eta, phi, e)\n",
    "    # use the objects from position 2 to 10\n",
    "    # position 0 and 1 are the initial state objects, with type == 0\n",
    "    # position 2 to 5 are b jets\n",
    "    # position 6 and 7 are light jets\n",
    "    # position 8 and 9 are leptons and MET\n",
    "    # position 10+ are spectator jets\n",
    "\n",
    "\n",
    "\n",
    "#bjets--> 2 to 5\n",
    "#light jets --> 6 and 7\n",
    "#rest--> 8,9,10"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sortable_cols = ['ME_Sig_Obj_pt', 'ME_Sig_Obj_eta', 'ME_Sig_Obj_phi', 'ME_Sig_Obj_E', 'ME_Sig_Obj_type']\n",
    "def sortby(column):\n",
    "    unsorted_dataframe = pd.DataFrame(data=higgs_signal_df)[::]\n",
    "    unsorted_dataframe = unsorted_dataframe[0:5]\n",
    "    sorted_dataframe = unsorted_dataframe[::]\n",
    "    bjets = \n",
    "    for col in sortable_cols:\n",
    "        for n in range(len(unsorted_dataframe)):\n",
    "            sorter = np.argsort(higgs_signal_df[column][n])\n",
    "            col_position = higgs_signal_df.columns.get_loc(hcol)\n",
    "            sorted_col = [[higgs_signal_df.iloc[n,col_position][sorter[::]]]]\n",
    "            sorted_dataframe.iloc[n,col_position] = sorted_col[::]\n",
    "    return sorted_dataframe\n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#no_spectators replaces args.no_spectators\n",
    "no_spectators = False\n",
    "# -- to remove spectator jets\n",
    "if no_spectators is True:\n",
    "    higgs_slice = np.logical_and(\n",
    "        higgs_slice, \n",
    "        np.array([(-1 not in event) for event in higgs_signal_df[\"ME_Sig_Obj_type\"]])\n",
    "        )\n",
    "    ttbar_slice = np.logical_and(\n",
    "        ttbar_slice, \n",
    "        np.array([(-1 not in event) for event in ttbar_signal_df[\"ME_Sig_Obj_type\"]])\n",
    "        )\n",
    "    \n",
    "higgs_signal_df_nospec = higgs_signal_df[higgs_slice]\n",
    "ttbar_signal_df_nospec = ttbar_signal_df[ttbar_slice]\n",
    "higgs_bkg_df_nospec = higgs_bkg_df[higgs_slice]\n",
    "ttbar_bkg_df_nospec = ttbar_bkg_df[ttbar_slice]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "log('applying lepton pt cut')\n",
    "# indices of Rows To Be Removed\n",
    "rtbr = [n for n, ev in ttbar_signal_df.iterrows() if ev['ME_Sig_Obj_pt'][np.logical_or(\n",
    "        ev['ME_Sig_Obj_type'] == 11,\n",
    "        ev['ME_Sig_Obj_type'] == 13)] < 20]\n",
    "\n",
    "print 'rtbr', rtbr\n",
    "ttbar_signal_df.drop(rtbr, inplace=True)\n",
    "ttbar_bkg_df.drop(rtbr, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-----\n",
    "def build_soft_target(sig_df, bkg_df, alpha=0.1):\n",
    "    ME_Bkg = np.array([a[0] for a in bkg_df['ME_Bkg_Mod_Likelihoods']])\n",
    "    ME_Sig = np.array([a[0] for a in sig_df['ME_Sig_Mod_Likelihoods']])\n",
    "    soft_target = ME_Sig / (ME_Sig + alpha * ME_Bkg)\n",
    "    return soft_target\n",
    "\n",
    "\n",
    "print 'ttbar_signal_df.shape', ttbar_signal_df.shape\n",
    "print 'ttbar_signal_df type', type(ttbar_signal_df)\n",
    "\n",
    "print 'ttbar_bkg_df.shape', ttbar_bkg_df.shape\n",
    "print 'ttbar_bkg_df type', type(ttbar_bkg_df)\n",
    "\n",
    "\n",
    "\n",
    "log('building soft_target')\n",
    "bkg_llh = np.log(np.array(\n",
    "    [a[0] for a in higgs_bkg_df['ME_Bkg_Mod_Likelihoods']] + [a[0] for a in ttbar_bkg_df['ME_Bkg_Mod_Likelihoods']]\n",
    "))\n",
    "signal_llh = np.log(np.array(\n",
    "    [a[0] for a in higgs_signal_df['ME_Sig_Mod_Likelihoods']] + [a[0] for a in ttbar_signal_df['ME_Sig_Mod_Likelihoods']]\n",
    "))\n",
    "if data == 'reco':\n",
    "    alpha = 0.1\n",
    "else:\n",
    "    alpha = 0.001\n",
    "soft_target = np.array(\n",
    "    build_soft_target(higgs_signal_df, higgs_bkg_df, alpha).tolist() + \n",
    "    build_soft_target(ttbar_signal_df, ttbar_bkg_df, alpha).tolist())\n",
    "\n",
    "del higgs_bkg_df, ttbar_bkg_df\n",
    "#-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log('building y')\n",
    "y = np.array(([1] * higgs_signal_df.shape[0]) + ([0] * ttbar_signal_df.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log('building X')\n",
    "df = pd.concat([higgs_signal_df, ttbar_signal_df], ignore_index=True)\n",
    "print 'df shape', np.shape(df)\n",
    "\n",
    "# build X from those 4 branches only (pt, eta, phi, e)\n",
    "# use the objfgects from position 2 to 10\n",
    "# position 0 and 1 are the initial state objects, with type == 0\n",
    "# position 2 to 5 are b jets\n",
    "# position 6 and 7 are light jets\n",
    "# position 8 and 9 are leptons and MET\n",
    "# position 10+ are spectator jets\n",
    "X = np.array([np.asarray(row.tolist())[:, 2:10].ravel() \n",
    "    for row in tqdm(df[['ME_Sig_Obj_pt', 'ME_Sig_Obj_eta', 'ME_Sig_Obj_phi', 'ME_Sig_Obj_E']].values)])\n",
    "X_full = X\n",
    "print X     \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log('building X')\n",
    "df = pd.concat([higgs_signal_df, ttbar_signal_df], ignore_index=True)\n",
    "print 'df shape', np.shape(df)\n",
    "\n",
    "# build X from those 4 branches only (pt, eta, phi, e)\n",
    "# use the objects from position 2 to 10\n",
    "# position 0 and 1 are the initial state objects, with type == 0\n",
    "# position 2 to 5 are b jets\n",
    "# position 6 and 7 are light jets\n",
    "# position 8 and 9 are leptons and MET\n",
    "# position 10+ are spectator jets\n",
    "    for row in tqdm(df[['ME_Sig_Obj_pt', 'ME_Sig_Obj_eta', 'ME_Sig_Obj_phi', 'ME_Sig_Obj_E']].values)])\n",
    "X_full = X\n",
    "print X     \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log('building the four vector')\n",
    "#df = pd.concat([higgs_signal_df, ttbar_signal_df], ignore_index=True)\n",
    "#print 'df shape', np.shape(df)\n",
    "#print df\n",
    "four_vector_and_type=df.loc[:,['ME_Sig_Obj_pt', 'ME_Sig_Obj_eta','ME_Sig_Obj_phi','ME_Sig_Obj_E','ME_Sig_Obj_type']]\n",
    "\n",
    "four_vector = four_vector_and_type.loc[:,'ME_Sig_Obj_pt':'ME_Sig_Obj_E']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "four_vector\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X=pd.concat(four_vector,df.loc[:,'ME_Sig_Obj_type'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X.iloc[0,:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "four_v=[]\n",
    "offset=8\n",
    "for i in range(8):\n",
    "    four_v.append([x[i],x[i+offset],x[i+2*offset],x[i+3*offset]])\n",
    "\n",
    "def sort_four_v(four_v):\n",
    "    v=np.array(four_v)\n",
    "    i=np.argsort(v[0])\n",
    "    return v[i]\n",
    "    \n",
    "sort_four_v(four_v[:4])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "quarter_len=len(x)/4\n",
    "\n",
    "xx=x.reshape(4,quarter_len).transpose()\n",
    "def sortv(v):\n",
    "    i=np.argsort(v[:,0])\n",
    "    return v[i]\n",
    "\n",
    "newv=np.concatenate([sortv(xx[0:4]),sortv(xx[4:6]),xx[6:]])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "newv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "sorter = np.concatenate([sortv(xx[0:4]),sortv(xx[4:6]),xx[6:]])[:,0]\n",
    "print sorter"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def sorted_by_pt(cell_number):\n",
    "    x=four_vector_and_type[cell_number]\n",
    "    four_v=[]\n",
    "    print x\n",
    "    offset=len(x)/4\n",
    "    for i in range(len(x)):\n",
    "        four_v.append([x[i],x[i+offset],x[i+2*offset],x[i+3*offset]])\n",
    "    sort_four_v(four_v[:4])\n",
    "    xx=x.reshape(4,quarter_len).transpose()\n",
    "    sorted_cell=np.concatenate([sortv(xx[0:4]),sortv(xx[4:6]),xx[6:]])[:,0]  \n",
    "    sorting_arg = np.argsort(sorted_cell)\n",
    "    return sorted_cell,sorting_arg"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "sorted_by_pt(1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "[np.argsort(sorted_by_pt(1))]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sorted_by_pt(1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n=0\n",
    "sorter = np.argsort(higgs_signal_df['ME_Sig_Obj_pt'][n])\n",
    "print sorter"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sorter= sorter+2\n",
    "print sorter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sortable_cols = ['ME_Sig_Obj_pt', 'ME_Sig_Obj_eta', 'ME_Sig_Obj_phi', 'ME_Sig_Obj_E', 'ME_Sig_Obj_type']\n",
    "def sortby(column):\n",
    "    unsorted_dataframe = four_vector_and_type.DataFrame(data=higgs_signal_df)[::]\n",
    "    unsorted_dataframe = unsorted_dataframe[0:5]\n",
    "    sorted_dataframe = unsorted_dataframe[::]\n",
    "    for col in sortable_cols:\n",
    "        for n in range(len(unsorted_dataframe)):\n",
    "            sorter = np.concatenate(0,1,(sorted_by_pt(n)+2))\n",
    "            print sorter\n",
    "            col_position = higgs_signal_df.columns.get_loc(hcol)\n",
    "            sorted_col = [[higgs_signal_df.iloc[n,col_position][sorter[::]]]]\n",
    "            sorted_dataframe.iloc[n,col_position] = sorted_col[::]\n",
    "    return sorted_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sortable_cols = ['ME_Sig_Obj_pt', 'ME_Sig_Obj_eta', 'ME_Sig_Obj_phi', 'ME_Sig_Obj_E', 'ME_Sig_Obj_type']\n",
    "def sortby(column):\n",
    "    unsorted_dataframe = four_vector.DataFrame(data=higgs_signal_df)[::]\n",
    "    sorted_dataframe = unsorted_dataframe[::]\n",
    "    for col in sortable_cols:\n",
    "        #for n in tqdm(range(len(unsorted_dataframe))):\n",
    "        for n in tqdm(range(2)):\n",
    "            print n\n",
    "            sorter = (sorted_by_pt(n))[1]\n",
    "            print sorter\n",
    "            new_sorter = np.append((0,1),sorter+2)\n",
    "            print new_sorter\n",
    "            col_position = unsorted_dataframe.columns.get_loc(col)\n",
    "            print col_position\n",
    "            col_to_sort = unsorted_dataframe.iloc[n,col_position]\n",
    "            print col_to_sort\n",
    "            sorted_col = col_to_sort[new_sorter]\n",
    "            print sorted_col\n",
    "            print np.shape(unsorted_dataframe)\n",
    "            print sorted_dataframe\n",
    "            print sorted_dataframe.iloc[n,col_position]\n",
    "            print sorted_col\n",
    "            print np.shape(sorted_dataframe.iloc[n,col_position])\n",
    "            print np.shape(sorted_col)\n",
    "            entries= len(sorted_col)\n",
    "            print sorted_dataframe.iloc[n,col_position][:20]\n",
    "            sorted_dataframe.iloc[n,col_position][:entries] = sorted_col[:]\n",
    "    return sorted_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sortable_cols = ['ME_Sig_Obj_pt', 'ME_Sig_Obj_eta', 'ME_Sig_Obj_phi', 'ME_Sig_Obj_E', 'ME_Sig_Obj_type']\n",
    "def sortby(column):\n",
    "    unsorted_dataframe = pd.DataFrame(data=higgs_signal_df)[::]\n",
    "    sorted_dataframe = unsorted_dataframe[::]\n",
    "    for col in sortable_cols:\n",
    "        #for n in tqdm(range(len(unsorted_dataframe))):\n",
    "        for n in tqdm(range(2)):\n",
    "            print n\n",
    "            sorter = (sorted_by_pt(n))[1]\n",
    "            print sorter\n",
    "            new_sorter = np.append((0,1),sorter+2)\n",
    "            print new_sorter\n",
    "            col_position = unsorted_dataframe.columns.get_loc(col)\n",
    "            print col_position\n",
    "            col_to_sort = unsorted_dataframe.iloc[n,col_position]\n",
    "            print col_to_sort\n",
    "            sorted_col = col_to_sort[new_sorter]\n",
    "            print sorted_col\n",
    "            print np.shape(unsorted_dataframe)\n",
    "            print sorted_dataframe\n",
    "            print sorted_dataframe.iloc[n,col_position]\n",
    "            print sorted_col\n",
    "            print np.shape(sorted_dataframe.iloc[n,col_position])\n",
    "            print np.shape(sorted_col)\n",
    "            entries= len(sorted_col)\n",
    "            print sorted_dataframe.iloc[n,col_position][:20]\n",
    "            sorted_dataframe.iloc[n,col_position][:entries] = sorted_col[:]\n",
    "    return sorted_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sortby('ME_Sig_Obj_pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replacing_column(unsorted_dataframe,replacement,position):\n",
    "    sorted_dataframe = unsorted_dataframe[::]\n",
    "    for n in tqdm(range(len(sorted_dataframe))):\n",
    "        entries= len(sorted_col)\n",
    "        sorted_dataframe.iloc[n,position][:entries] = sorted_col[:]\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unsorted_dataframe = pd.DataFrame(data=higgs_signal_df)[::]\n",
    "pt_column='ME_Sig_Obj_pt'\n",
    "replacing_column(unsorted_dataframe,new_col,pt_column)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " log('shuffling, splitting and scaling')\n",
    "    X_train, X_test, y_train, y_test, soft_target_train, soft_target_test,\\\n",
    "    bkg_llh_train, bkg_llh_test, signal_llh_train, signal_llh_test = train_test_split(\n",
    "            X, y, soft_target, bkg_llh, signal_llh, train_size=0.75\n",
    "    )\n",
    "\n",
    "    X_test, X_eval, y_test, y_eval, soft_target_test, soft_target_eval,\\\n",
    "    bkg_llh_test, bkg_llh_eval, signal_llh_test, signal_llh_eval= train_test_split(\n",
    "        X_test, y_test, soft_target_test, bkg_llh_test, signal_llh_test, train_size=0.15\n",
    "    )\n",
    "\n",
    "    # 0.5 of original data will be in dataset A, \n",
    "    # (was 0.5 * 0.6 = 0.3 --> now 0.25 * 0.15 = 0.0375) of the data will be in dataset B, \n",
    "    # and (was 0.5 * 0.4 = 0.2 --> now 0.25 * 0.85 = 0.2125) of the data will be in dataset C \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  #log('plotting unscaled input variables')\n",
    "    #safe_mkdir(os.path.join('plots', data))\n",
    "    #plot_distributions(X_train, y_train, soft_target_train, bkg_llh_train, signal_llh_train,\\\n",
    "    #    X_test, y_test, soft_target_test, bkg_llh_test, signal_llh_test,\\\n",
    "    #    X_eval, y_eval, soft_target_eval, bkg_llh_eval, signal_llh_eval, '_unscaled', data)\n",
    "    #print 'unscaled input variables plotted'\n",
    "\n",
    "    scaler = RobustScaler(quantile_range=(25, 75))\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    X_eval = scaler.transform(X_eval)\n",
    "\n",
    "    #log('plotting scaled input variables')\n",
    "    #plot_distributions(X_train, y_train, soft_target_train, bkg_llh_train, signal_llh_train,\\\n",
    "    #    X_test, y_test, soft_target_test, bkg_llh_test, signal_llh_test,\\\n",
    "    #    X_eval, y_eval, soft_target_eval, bkg_llh_eval, signal_llh_eval, '_scaled', data)\n",
    "    #print 'scaled input variables plotted'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    # -- save scaling parameters to be used by lwtnn converter\n",
    "    log('saving scaling options to ' + os.path.join(SAMPLE_PATH, 'variables.json'))\n",
    "    var_names = [\n",
    "        'pT_b-jet_1', 'pT_b-jet_2', 'pT_b-jet_3', 'pT_b-jet_4', 'pT_l-jet_1', 'pT_l-jet_2', 'pT_lepton', 'pT_MET', \n",
    "        'eta_b-jet_1', 'eta_b-jet_2', 'eta_b-jet_3', 'eta_b-jet_4', 'eta_l-jet_1', 'eta_l-jet_2', 'eta_lepton', 'eta_MET',\n",
    "        'phi_b-jet_1', 'phi_b-jet_2', 'phi_b-jet_3', 'phi_b-jet_4', 'phi_l-jet_1', 'phi_l-jet_2', 'phi_lepton', 'phi_MET',\n",
    "        'E_b-jet_1', 'E_b-jet_2', 'E_b-jet_3', 'E_b-jet_4', 'E_l-jet_1', 'E_l-jet_2', 'E_lepton', 'E_MET']\n",
    "    # -- write variable json for lwtnn keras2json converter\n",
    "    variable_dict = {\n",
    "        'inputs': [{\n",
    "            'name': var_names[v],\n",
    "            'scale': float(1.0 / scaler.scale_[v]),\n",
    "            #'offset': float(-scaler.mean_[v]),\n",
    "            'default': None\n",
    "            } \n",
    "            for v in xrange(len(var_names))],\n",
    "        'class_labels': ['bkg_llh', 'signal_llh'],\n",
    "        'keras_version': keras.__version__\n",
    "    }\n",
    "    with open(os.path.join(SAMPLE_PATH, 'variables.json'), 'wb') as varfile:\n",
    "        json.dump(variable_dict, varfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    # -- save data archives\n",
    "    train_ = {\n",
    "        'X' : X_train,\n",
    "        'y' : y_train,\n",
    "        #'soft_target' : soft_target_train,\n",
    "        #'bkg_llh' : bkg_llh_train,\n",
    "        'signal_llh' : signal_llh_train\n",
    "    }\n",
    "\n",
    "    test_ = {\n",
    "        'X' : X_test,\n",
    "        'y' : y_test,\n",
    "        #'soft_target' : soft_target_test,\n",
    "        #'bkg_llh' : bkg_llh_test,\n",
    "        'signal_llh' : signal_llh_test\n",
    "    }\n",
    "\n",
    "    eval_ = {\n",
    "        'X' : X_eval,\n",
    "        'y' : y_eval,\n",
    "        #'soft_target' : soft_target_eval,\n",
    "        #'bkg_llh' : bkg_llh_eval,\n",
    "        'signal_llh' : signal_llh_eval\n",
    "    }\n",
    "\n",
    "    log('saving to h5 archive...')\n",
    "    spectators = '_NO_SPECTATORS' if no_spectators else ''\n",
    "    io.save(os.path.join(SAMPLE_PATH, 'MEM-A-0500' + spectators + '.h5'), train_)\n",
    "    io.save(os.path.join(SAMPLE_PATH, 'MEM-B-0500' + spectators +'.h5'), test_)\n",
    "    io.save(os.path.join(SAMPLE_PATH, 'MEM-C-0500' + spectators +'.h5'), eval_)\n",
    "    print (os.path.join(SAMPLE_PATH, 'MEM-A-0500' + spectators + '.h5'), train_)\n",
    "    print (os.path.join(SAMPLE_PATH, 'MEM-B-0500' + spectators +'.h5'), test_)\n",
    "    print (os.path.join(SAMPLE_PATH, 'MEM-C-0500' + spectators +'.h5'), eval_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "    # def shape_matched_from_df(cls, df, ids):\n",
    "    #     t = df.shape[0] - 1\n",
    "    #     particles = []\n",
    "    #     # for i, (_, p) in enumerate(df[ids].iterrows()):\n",
    "    #     for i, p in enumerate(zip(*[df[_id].values for _id in ids])):\n",
    "    #         if i % 1000 == 0:\n",
    "    #             sys.stdout.write('\\r{:.2f}% complete.'.format(100 * (float(i) / t)))\n",
    "    #             sys.stdout.flush()\n",
    "    #         particles.append([cls(*v) for v in zip(*p)])\n",
    "    #     sys.stdout.write('\\nDone.')\n",
    "    #     return particles\n",
    "    #     # return pup.match_shape(np.array(particles), df[ids[0]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # log('Starting particle matching using wrapper classes.')\n",
    "    # # -- build shape matched lists of particles...\n",
    "\n",
    "    # jet_features = ['jet_pt', 'jet_eta', 'jet_phi', 'jet_E']\n",
    "    # lepton_features = ['lep_pt', 'lep_eta', 'lep_phi', 'lep_E']\n",
    "\n",
    "    # log('Shape matching background jets...')\n",
    "    # bkg_jets = shape_matched_from_df(Jet, bkg_df, jet_features)\n",
    "\n",
    "    # log('Shape matching background leptons...')\n",
    "    # bkg_leps = shape_matched_from_df(Lepton, bkg_df, lepton_features)\n",
    "\n",
    "    # log('Shape matching signal jets...')\n",
    "    # signal_jets = shape_matched_from_df(Jet, signal_df, jet_features)\n",
    "\n",
    "    # log('Shape matching signal leptons...')\n",
    "    # signal_leps = shape_matched_from_df(Lepton, signal_df, lepton_features)\n",
    "\n",
    "\n",
    "    # log('Loading old dataframes')\n",
    "    # # -- load in pre-saved dataframes with all other variables\n",
    "    # old_bkg_df = pd.read_pickle(os.path.join(SAMPLE_PATH, 'bkg_df.pkl'))\n",
    "    # old_signal_df = pd.read_pickle(os.path.join(SAMPLE_PATH, 'signal_df.pkl'))\n",
    "\n",
    "    # log('Adding energy to old dataframes')\n",
    "    # -- add the jet energy to the dataframe\n",
    "    # old_bkg_df['jet_E'] = bkg_df['jet_E']\n",
    "    # old_signal_df['jet_E'] = signal_df['jet_E']\n",
    "\n",
    "    # -- add the jet and lepton 4-vectors to the dataframe\n",
    "    # log('Adding newly computed quantities to old dataframes')\n",
    "    # bkg_df['jet_4v'] = bkg_jets\n",
    "    # signal_df['jet_4v'] = signal_jets\n",
    "    # bkg_df['lep_4v'] = bkg_leps\n",
    "    # signal_df['lep_4v'] = signal_leps\n",
    "\n",
    "    # # -- get some truth up in here\n",
    "    # bkg_df['truth'] = 0\n",
    "    # signal_df['truth'] = 1\n",
    "\n",
    "    # # -- get the permutations\n",
    "    # log('Grabbing background permutations')\n",
    "    # bkg_file = ROOT.TFile(os.path.join(SAMPLE_PATH, BKG_FILE_PATTERN))\n",
    "\n",
    "    # bkg_tree = bkg_file.Get(BKG_TREE)\n",
    "\n",
    "    # bkg_perm = []\n",
    "    # nb_entries = bkg_tree.GetEntries()\n",
    "\n",
    "    # log('fetching permutations')\n",
    "\n",
    "    # for i in xrange(nb_entries):\n",
    "    #     bkg_tree.GetEntry(i)\n",
    "    #     sp = []\n",
    "    #     for j in xrange(bkg_tree.ME_Sig_Mod_Permutations[0][0].size()):\n",
    "    #         sp.append(int(bkg_tree.ME_Sig_Mod_Permutations[0][0][j]))\n",
    "    #     bkg_perm.append(sp[:-2])\n",
    "\n",
    "    # bkg_df['jet_order'] = bkg_perm\n",
    "\n",
    "    # log('Grabbing signal permutations')\n",
    "    \n",
    "    # signal_file = ROOT.TFile(os.path.join(SAMPLE_PATH, SIGNAL_FILE_PATTERN))\n",
    "\n",
    "    # signal_tree = signal_file.Get(SIGNAL_TREE)\n",
    "\n",
    "    # signal_perm = []\n",
    "    # nb_entries = signal_tree.GetEntries()\n",
    "\n",
    "    # log('fetching permutations')\n",
    "\n",
    "    # for i in xrange(nb_entries):\n",
    "    #     signal_tree.GetEntry(i)\n",
    "    #     sp = []\n",
    "    #     for j in xrange(signal_tree.ME_Sig_Mod_Permutations[0][0].size()):\n",
    "    #         sp.append(int(signal_tree.ME_Sig_Mod_Permutations[0][0][j]))\n",
    "    #     signal_perm.append(sp[:-2])\n",
    "\n",
    "    # signal_df['jet_order'] = signal_perm\n",
    "\n",
    "    # # -- get the weights\n",
    "    # log('Grabbing event weights')\n",
    "    # bkg_df['evt_weight'] = [a[4] * b * c for (a, b, c) in zip(bkg_df['TRFMCweight_in'], bkg_df['ttbarTopPtDataWeight'], bkg_df['LeptonSF'])]\n",
    "    # signal_df['evt_weight'] = [(a[4] * b) / c for (a, b, c) in zip(signal_df['TRFMCweight_in'], signal_df['LeptonSF'], signal_df['ttH_NLO_TruthWeight'])]\n",
    "\n",
    "    # log('applying preselections')\n",
    "    # # -- cut according to Olaf's instructions\n",
    "    # bkg_cut = (bkg_df['ME_Bkg_Likelihoods_Available'] == 1) & \\\n",
    "    #           (bkg_df['ME_Sig_Likelihoods_Available'] == 1)\n",
    "\n",
    "    # signal_cut = (signal_df['ME_Bkg_Likelihoods_Available'] == 1) & \\\n",
    "    #              (signal_df['ME_Sig_Likelihoods_Available'] == 1)\n",
    "\n",
    "    # log('concatenating signal and background')\n",
    "    # # -- concatenate signal and background df's\n",
    "    # df = pd.concat(\n",
    "    #     [bkg_df[bkg_cut], signal_df[signal_cut]], \n",
    "    #     ignore_index=True\n",
    "    # )\n",
    "\n",
    "    # # need to reorder jets according to the analysis's scheme\n",
    "    # # first 4 are b-tagged, other 2 are leading untagged jets\n",
    "    # # this will give you the indices\n",
    "\n",
    "    # log('getting reordering')\n",
    "    # reordering = np.array(df['jet_order'])\n",
    "\n",
    "    # log('reordering particles')\n",
    "    # df['ordered_jet_4v'] = [np.array(p)[ix].tolist() for p, ix in zip(df['jet_4v'], reordering)]\n",
    "\n",
    "    # # -- list of dot products among jets (21 per event)\n",
    "    # log('Generating dot products...')\n",
    "    # dot_prod_list = [\n",
    "    #                     [\n",
    "    #                         (a.lv + b.lv).M() #a.lv.Dot(b.lv) \n",
    "    #                         for i, a in enumerate(e) \n",
    "    #                             for j, b in enumerate(e) \n",
    "    #                                 if j >= i\n",
    "    #                     ] \n",
    "    #                     for e in df['ordered_jet_4v']\n",
    "    #                 ]\n",
    "\n",
    "\n",
    "\n",
    "    # log('Making X and y')\n",
    "\n",
    "    # y = np.array(df['truth'])\n",
    "\n",
    "    # N_COL = 23 \n",
    "\n",
    "    # if not ONLY_4V:\n",
    "    #     N_COL = N_COL + 21 + 5  # regular variables + dot products + other features\n",
    "\n",
    "    # X = np.zeros((df.shape[0], N_COL))\n",
    "\n",
    "    # log('adding lepton info')\n",
    "\n",
    "    # X[:, 0] = df['lep_pt'];\n",
    "    # X[:, 0] = np.log(X[:, 0]) \n",
    "    # X[:, 1] = df['lep_eta'];\n",
    "    # X[:, 2] = df['lep_phi'];\n",
    "    # X[:, 3] = df['metx'];\n",
    "    # X[:, 4] = df['mety'];\n",
    "\n",
    "\n",
    "    # log('adding jet info')\n",
    "    # PT_COLUMN = 5\n",
    "    # N_JETS = 6\n",
    "\n",
    "    # ETA_COLUMN = PT_COLUMN + N_JETS\n",
    "    # PHI_COLUMN = ETA_COLUMN + N_JETS\n",
    "           \n",
    "    # X[:, PT_COLUMN : (PT_COLUMN + N_JETS)] = [np.log(np.array(e))[ix].tolist() for e, ix in zip(df['jet_pt'], reordering)]\n",
    "\n",
    "    # X[:, ETA_COLUMN : (ETA_COLUMN + N_JETS)] = [np.array(e)[ix].tolist() for e, ix in zip(df['jet_eta'], reordering)]\n",
    "\n",
    "    # X[:, PHI_COLUMN : (PHI_COLUMN + N_JETS)] = [np.array(e)[ix].tolist() for e, ix in zip(df['jet_phi'], reordering)]\n",
    "\n",
    "\n",
    "    # if not ONLY_4V:\n",
    "    #     log('adding dot products')\n",
    "    #     # -- add the dot products...\n",
    "    #     DOT_COLUMN = PHI_COLUMN + N_JETS\n",
    "    #     FEATURE_COLUMN = (DOT_COLUMN + 21)\n",
    "\n",
    "    #     X[:, DOT_COLUMN : FEATURE_COLUMN] = np.array(dot_prod_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #     log('adding secondary features')\n",
    "    #     # -- angle between the first and second b jets\n",
    "    #     X[:, FEATURE_COLUMN] = [e[0].lv.Angle(e[1].lv) for e in df['ordered_jet_4v']]\n",
    "\n",
    "    #     # -- angle between the light jets\n",
    "    #     X[:, FEATURE_COLUMN + 1] = [e[4].lv.Angle(e[5].lv) for e in df['ordered_jet_4v']]\n",
    "\n",
    "    #     # -- decay angle of the first jet\n",
    "    #     X[:, FEATURE_COLUMN + 2] = [e[0].lv.CosTheta() for e in df['ordered_jet_4v']]\n",
    "\n",
    "    #     # -- mass of reconstructed bb+qq+b system (hopefully similar to top mass?)\n",
    "    #     X[:, FEATURE_COLUMN + 3] = [(e[0].lv + e[1].lv + e[2].lv + e[4].lv + e[5].lv).M() / 1000 for e in df['ordered_jet_4v']]\n",
    "\n",
    "    #     # -- qq product, should find W mass...\n",
    "    #     X[:, FEATURE_COLUMN + 4] = [(e[4].lv + e[5].lv).M() for e in df['ordered_jet_4v']]\n",
    "\n",
    "    # # -- hard target\n",
    "    # y = df['truth'].values\n",
    "\n",
    "    # log('scaling data')\n",
    "    # scaler = StandardScaler()\n",
    "    # Z = scaler.fit_transform(np.array(X))\n",
    "\n",
    "    # soft_target = (df['ME_Sig_Mod_Likelihoods'] / (df['ME_Sig_Mod_Likelihoods'] + 0.23 * df['ME_Bkg_Mod_Likelihoods']))\n",
    "    # soft_target = np.array([e[0] for e in soft_target.values])\n",
    "\n",
    "    # w = df.evt_weight.values\n",
    "    # # w = (w * (len(w) / w.sum())).values\n",
    "\n",
    "    # n = Z.shape[0]\n",
    "    # ix = range(n)\n",
    "    # np.random.shuffle(ix)\n",
    "\n",
    "    # log('determining train/test/val splits')\n",
    "    # tr, te, val = ix[:(n / 2)], ix[(n / 2) : int((n / 2) + 0.18 * n)], ix[int((n / 2) + 0.18 * n):]\n",
    "\n",
    "    # splits = {'train' : np.array(tr), 'test' :  np.array(te), 'validation' : np.array(val)}\n",
    "\n",
    "    # data = {\n",
    "    #     'Z' : Z,\n",
    "    #     'y' : y,\n",
    "    #     'soft_target' : soft_target,\n",
    "    #     'weights' : w,\n",
    "    #     'splits' : splits\n",
    "    # }\n",
    "\n",
    "    # log('saving to h5 archive...')\n",
    "    # io.save(os.path.join(SAMPLE_PATH, 'MEM-db.h5'), data)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "datasetA = io.load('MEM-A-0500.h5')\n",
    "datasetB = io.load('MEM-B-0500.h5')\n",
    "datasetC = io.load('MEM-C-0500.h5')\n",
    "datasets = [datasetA, datasetB, datasetC]\n",
    "print 'datasetA', datasetA.keys()\n",
    "print 'datasetB', datasetB.keys()\n",
    "print 'datasetC', datasetC.keys()\n",
    "\n",
    "np.random.seed(0)\n",
    "print np.random.rand(1)\n",
    "\n",
    "train =  [datasetA.items[np.random.choice(len(datasetA.values), size=5000, replace = False)]] \n",
    "print np.shape(test)\n",
    "print np.shape(train)\n",
    "               \n",
    "test =  [datasetB[np.random.choice(len(datasetB), size=3000, replace = False)]] \n",
    "print np.shape(test)\n",
    "\n",
    "eval =  [datasetC[np.random.choice(len(datasetC), size=2000, replace = False)]]\n",
    "print np.shape(eval)\n",
    "\n",
    "io.save('Samples_0500_train.h5', train)\n",
    "io.save('Samples_0500_test.h5', test)\n",
    "io.save('Samples_0500_eval.h5', eval)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
